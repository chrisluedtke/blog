{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiPy Mentorship 2 of 3\n",
    "\n",
    "#### Learning and Project Clarity\n",
    "Since Post 1, my time investment in this project has varied between learning data science at a high level and expanding my Python foundation through tutorials and project work. I have been working my way through the book, Data Science for Business, which nicely balances emphasis between high level discussion and gritty nuance. Importantly, the text provides language for speaking to my interests and experiences in analytics and defining my project stages and scope.\n",
    "\n",
    "The analysis I’m conducting involves profiling and causal modeling. I want to know which data points correlate most strongly to student outcomes and AmeriCorps Member performance (see my previous post for information on my role at an educational non-profit). I am hoping I can then build a profile of successful AmeriCorps Members (ACMs) and use this model to recommend metrics and goals that better align to student outcomes and ACM performance. Additionaly, this analysis could help advocate to school partners for the school settings in which our AmeriCorps Members best serve students’ individualistic needs.\n",
    "\n",
    "On the technical Python side, I have gained further experience with tuples, dictionaries, sets, and `with` control flows while adding further practice to the other basic types and `pandas` techniques I already knew well. The tutorials I followed include a Exploratory Data Analysis https://github.com/cmawer/pycon-2017-eda-tutorial from PyCon, the seaborn data visualization tutorial https://seaborn.pydata.org/tutorial.html, and a little natural langauge processing http://textblob.readthedocs.io/en/dev/.\n",
    "\n",
    "Although these tutorials filled me with a sense of potential and accompishment, the feeling quickly faded in the absence of application. Of course, this is where the 'hard learning' occurs and meaningful progress is made. Following are two examples of the data I have processed in my project to date.\n",
    "\n",
    "#### Getting Data\n",
    "First I needed to get my data into Python. The majority of my data are stored on Salesforce or Excel workbooks through SharePoint.\n",
    "\n",
    "Going into this project, I had already developed Python systems for getting and writing Excel documents to SharePoint. This was accomplished through a mapped network drive to the SharePoint server, allowing me to simply navigate the file structure as a local drive.\n",
    "\n",
    "The Salesforce component was a bit more tricky, but with a little Python elbow grease, the `simple-salesforce` package was up to the task. This package allows me to query, create, and delete records, which has had a profound effect on my work pracitices. In a typical use of simple-salesforce, the user passes a traditional SOQL query into the Salesforce instance (in this case `cysh`) and performs the `query_all` function.\n",
    "\n",
    "Here is the set-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from simple_salesforce import Salesforce\n",
    "\n",
    "with open('C:\\\\Users\\\\City_Year\\\\Desktop\\\\salesforce_credentials.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    sf_creds = eval(read_data)\n",
    "\n",
    "# our Salesforce instance is referred to as cyschoolhouse (cysh)\n",
    "cysh = Salesforce(instance_url=sf_creds['instance_url'],\n",
    "                  password=sf_creds['password'],\n",
    "                  username=sf_creds['username'],\n",
    "                  security_token=sf_creds['security_token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple query, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querystring = (f\"SELECT Id, Name FROM Assesment__c\") # Note the typo in Assesment__c, which cost me a good amount of time troubleshooting\n",
    "query_return = cysh.query_all(querystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(query_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_return.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_return['records'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, I'm working with datasets that are not very large. In the Chicago region, we serve fewer than 3000 students and enlist fewer than 300 AmeriCorps Members. Therefore I can query the entire Salesforce object (i.e. `Assesment__c`) with whatever fields I'm interested in and perform all the shaping and filtering in Python.\n",
    "\n",
    "I added this convenient function to iterate over the query response and shape it as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cysh_df(sf_object, sf_fields, rename_id=False, rename_name=False, sf=cysh):\n",
    "    sf_fields_str = \", \".join(sf_fields)\n",
    "    querystring = (f\"SELECT {sf_fields_str} FROM {sf_object}\")\n",
    "    query_return = cysh.query_all(querystring)\n",
    "\n",
    "    query_list = []\n",
    "    for row in query_return['records']:\n",
    "        record = []\n",
    "        for column in sf_fields:\n",
    "            col_data = row[column]\n",
    "            record.append(col_data)\n",
    "        query_list.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(query_list, columns=sf_fields)\n",
    "    \n",
    "    if rename_id==True:\n",
    "        df.rename(columns={'Id':sf_object}, inplace=True)\n",
    "    if rename_name==True:\n",
    "        df.rename(columns={'Name':(sf_object+'_Name')}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of this function is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a041a00000E9Y3xAAF</td>\n",
       "      <td>a041a00000E9Y3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a041a00000EmNs0AAF</td>\n",
       "      <td>a041a00000EmNs0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a041a00000EmOf8AAF</td>\n",
       "      <td>a041a00000EmOf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a041a00000FV7AaAAL</td>\n",
       "      <td>a041a00000FV7Aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a041a00000FVLdgAAH</td>\n",
       "      <td>a041a00000FVLdg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id             Name\n",
       "0  a041a00000E9Y3xAAF  a041a00000E9Y3x\n",
       "1  a041a00000EmNs0AAF  a041a00000EmNs0\n",
       "2  a041a00000EmOf8AAF  a041a00000EmOf8\n",
       "3  a041a00000FV7AaAAL  a041a00000FV7Aa\n",
       "4  a041a00000FVLdgAAH  a041a00000FVLdg"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessment_df = get_cysh_df('Assesment__c', ['Id', 'Name'])\n",
    "assessment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of assessment data, the next step will be determing student assessment progress by comparing prior year or start of year assessments to the recent winter assessment. Typically we set assessment goals fall to spring, so in this analysis I will need to calculate mid-year goals. Ideally, this analysis will also consider data from past years, but the feasbility of that is yet to be determined. To view my current progress on assessment data, view the IPython notebook in the repository for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data\n",
    "In my organization we employ four coaches who observe, provide feedback, and enter data for each AmeriCorps member at least once per month. If this data were cleaned and rolled together, I could determine whether the AmeriCorps Members who exercise best practices in tutoring are in fact more impactful with students (among many other interesting questions). The reason I put off this analysis is because the 26 Excel workbooks created to track coaching data were not designed with aggregation in mind. The records are indexed by first names and nicknames, headers are inconsistent (and multi-indexed), and the file structure does not reflect standardized organization.\n",
    "\n",
    "The first task was to identify the file paths to each Excel workbook. This was accomplished using `os.walk()` to list all contents of the relevant SharePoint directories. Since there are only 26 files, and the filenames are not patterned, I chose to simply copy-paste the file paths I wanted into a list.\n",
    "\n",
    "The next challenge was that these Excel workbooks do not contain consistent sheets. All workbooks contain months as sheet names, but some coaches split their workbook into two semesters. Instead of pandas' `pd.read_excel()`, I used `pd.ExcelFile()`. This allowed me to load the workbook once, then only pull the sheets whose names also occur in the set of sheets I'm interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in coach_xlsx_paths:\n",
    "    with pd.ExcelFile(filepath) as xl:\n",
    "        for sheet in list(set(xl.sheet_names) & set(sheet_months)):\n",
    "            # Returns ordered dict\n",
    "            heatmap = pd.read_excel(xl, header=[2,3], \n",
    "                                    sheet_name=sheet, \n",
    "                                    usecols=14, index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this process, I also added columns for the month (the sheet name) and the school (the directory in which the file resides), but again the school folder names were not standardized.\n",
    "\n",
    "Enter `fuzzywuzzy`, a very hand package for finding and scoring text matches. I performed fuzzy matches between the folder name columns and the official school names from Salesforce like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load schools from salesforce\n",
    "account_df = get_cysh_df('Account', ['Id', 'Name'])\n",
    "account_df.rename(columns={'Id':'Organization__c', 'Name':'School'}, inplace=True)\n",
    "\n",
    "d = []\n",
    "for folder_name in set(heatmaps_df['Folder']):\n",
    "    match = process.extract(folder_name, set(account_df['School']), scorer=fuzz.token_set_ratio, limit=1)\n",
    "    match = [folder_name] + [x for tup in match for x in tup]\n",
    "    d.append(match)\n",
    "df = pd.DataFrame(d, columns=['Folder', 'School', 'Match_Score'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same process, I was able to clean ACM names too. However, in this case there were several ambiguous matches, which I resolved by manually comparing names between the Excel workbook and the validated set of ACM names from Salesforce. I then fixed the mis-matched names in Excel by hand. You can also view this full process in the project repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "Next in my analysis will be cleaning and combining of 2-3 more data sources. Then I will conduct exploratory data analysis, cyclically chasing my branching curiosities as they come. For example, I will experiment with various definitions of ACM success to tentatively determine which features seem worthwhile to explore further. In combination with studying data science techniques, I am optimistic that I can build a model that describes signifiers of successful novel discoveries about our programming."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
