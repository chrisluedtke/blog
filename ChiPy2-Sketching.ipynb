{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiPy Mentorship 2 of 3\n",
    "\n",
    "Since Post 1 my project work has varied between learning data science at a high level, expanding my Python foundation, and conducting exploratory data analysis. In this post I walk through the tools I’m using and my approach to an evolving goal line.\n",
    "\n",
    "#### Learning and Project Clarity\n",
    "Since my last post I have been working my way through the book, Data Science for Business (thanks to my mentor Aly for the purchase). The book provides a nice balance between high level discussion and gritty nuance. Most importantly, the text provides language for speaking to my interests and experiences in analytics, and defining my the stages and scope of my project.\n",
    "\n",
    "See my previous post for information on my role as an analyst at an AmeriCorps non-profit and the data at my disposal.\n",
    "\n",
    "The analysis I’m conducting involves profiling and causal modeling. I want to know which data points correlate most strongly to student outcomes and AmeriCorps Member performance. I am hoping I can then build a profile of successful (or unsuccessful) AmeriCorps Members (ACMs) and use this model to recommend metrics and goals that better align to student outcomes and ACM performance. Additionaly, this analysis could help advocate to school partners for the school settings in which our AmeriCorps Members best serve students’ individualistic needs.\n",
    "\n",
    ", but offered an excellent opportunity to practice my Python fundamentals. The solution involed tuples, dictionaries, sets, `with` control flow structure, in addition to the list and pandas dataframe types I use most often\n",
    "\n",
    "#### Getting Data\n",
    "The first task was to get my data into Python. The majority of our data are stored on Salesforce or Excel workbooks through SharePoint. Going into this project, I had already developed Python systems for getting and writing Excel documents to SharePoint. This was accomplished through a mapped network drive to the SharePoint server, allowing me to simply navigate the file structure as a local drive.\n",
    "\n",
    "The Salesforce component was a bit more tricky. I relied heavily on the package, `simple-salesforce`. This package allows me to query, create, and delete records, which has had a profound effect on my work pracitices. In a typical use of simple-salesforce, the user passes a traditional SOQL query into the Salesforce instance `cysh` and performs the `query_all` function.\n",
    "\n",
    "Here is the set-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from simple_salesforce import Salesforce\n",
    "\n",
    "with open('C:\\\\Users\\\\City_Year\\\\Desktop\\\\salesforce_credentials.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    sf_creds = eval(read_data)\n",
    "\n",
    "# our Salesforce instance is referred to as cyschoolhouse (cysh)\n",
    "cysh = Salesforce(instance_url=sf_creds['instance_url'],\n",
    "                  password=sf_creds['password'],\n",
    "                  username=sf_creds['username'],\n",
    "                  security_token=sf_creds['security_token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a simple query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querystring = (f\"SELECT Id, Name FROM Assesment__c\") # note the typo in Assesment__c, which took too long to discover\n",
    "query_return = cysh.query_all(querystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(query_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_return.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_return['records'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, I'm not working with datasets that are very large. In Chicago, we serve fewer than 3000 students, and we enlist fewer than 300 AmeriCorps Members. Therefore I can query the entire Salesforce object, `Assesment__c`, with whatever fields I'm interested in, and perform all the shaping and filtering with pandas.\n",
    "\n",
    "I added this convenient function to iterate over the query response and shape it as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cysh_df(sf_object, sf_fields, rename_id=False, rename_name=False, sf=cysh):\n",
    "    sf_fields_str = \", \".join(sf_fields)\n",
    "    querystring = (f\"SELECT {sf_fields_str} FROM {sf_object}\")\n",
    "    query_return = cysh.query_all(querystring)\n",
    "\n",
    "    query_list = []\n",
    "    for row in query_return['records']:\n",
    "        record = []\n",
    "        for column in sf_fields:\n",
    "            col_data = row[column]\n",
    "            record.append(col_data)\n",
    "        query_list.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(query_list, columns=sf_fields)\n",
    "    \n",
    "    if rename_id==True:\n",
    "        df.rename(columns={'Id':sf_object}, inplace=True)\n",
    "    if rename_name==True:\n",
    "        df.rename(columns={'Name':(sf_object+'_Name')}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of this function is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a041a00000E9Y3xAAF</td>\n",
       "      <td>a041a00000E9Y3x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a041a00000EmNs0AAF</td>\n",
       "      <td>a041a00000EmNs0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a041a00000EmOf8AAF</td>\n",
       "      <td>a041a00000EmOf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a041a00000FV7AaAAL</td>\n",
       "      <td>a041a00000FV7Aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a041a00000FVLdgAAH</td>\n",
       "      <td>a041a00000FVLdg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id             Name\n",
       "0  a041a00000E9Y3xAAF  a041a00000E9Y3x\n",
       "1  a041a00000EmNs0AAF  a041a00000EmNs0\n",
       "2  a041a00000EmOf8AAF  a041a00000EmOf8\n",
       "3  a041a00000FV7AaAAL  a041a00000FV7Aa\n",
       "4  a041a00000FVLdgAAH  a041a00000FVLdg"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessment_df = get_cysh_df('Assesment__c', ['Id', 'Name'])\n",
    "assessment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view my current progress on getting assessment data and my next steps in shaping and analysis, view the IPython notebooks in the repository for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data\n",
    "In my organization we employ four coaches who observe, provide feedback, and enter data for each AmeriCorps member at least once per month. If this data were cleaned and rolled together, I could determine whether the AmeriCorps Members who exercise best practices in tutoring are in fact more impactful with students (among many other interesting questions). The reason I put off this analysis is because the 26 Excel workbooks created to track coaching data were not designed with aggregation in mind. The records are indexed by first names and nicknames, headers are inconsistent (and multi-indexed), and the file structure does not reflect standardized organization.\n",
    "\n",
    "The first task was to identify the file paths to each Excel workbook. This was accomplished using `os.walk()` to list all contents of the relevant directories. Since there are only 26 files, and the filenames are not patterned, I chose to simply copy-paste the file paths I wanted into a list.\n",
    "\n",
    "The next challenge was that these Excel workbooks do not contain consistent sheets. All workbooks contain months as sheet names, but some coaches split their workbook into two semesters. Instead of pandas' `pd.read_excel()`, I used `pd.ExcelFile()`. This allowed me to load the workbook, then only load the sheets whose names also exist in the set of sheets I'm interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in coach_xlsx_paths:\n",
    "    with pd.ExcelFile(filepath) as xl:\n",
    "        for sheet in list(set(xl.sheet_names) & set(sheet_months)):\n",
    "            # Returns ordered dict\n",
    "            heatmap = pd.read_excel(xl, header=[2,3], \n",
    "                                    sheet_name=sheet, \n",
    "                                    usecols=14, index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this process, I also added columns for the month (the sheet name) and the school (the directory in which the file resides). Unfortunately the school folder names were also not standardized.\n",
    "\n",
    "Enter `fuzzywuzzy`, a very hand package for finding and scoring text matches. I performed fuzzy matches between the folder name columns and the official school names from salesfroce: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load schools from salesforce\n",
    "account_df = get_cysh_df('Account', ['Id', 'Name'])\n",
    "account_df.rename(columns={'Id':'Organization__c', 'Name':'School'}, inplace=True)\n",
    "\n",
    "d = []\n",
    "for folder_name in set(heatmaps_df['Folder']):\n",
    "    match = process.extract(folder_name, set(account_df['School']), scorer=fuzz.token_set_ratio, limit=1)\n",
    "    match = [folder_name] + [x for tup in match for x in tup]\n",
    "    d.append(match)\n",
    "df = pd.DataFrame(d, columns=['Folder', 'School', 'Match_Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O&C data\n",
    "- fuzzywuzzy to flag names to be manually fixed\n",
    "\n",
    "#### Next Steps\n",
    "Next I will determine student assessment progress by comparing prior year or start of year assessments to the recent winter assessment. Typically we set assessment goals fall to spring, so in this analysis I will need to calculate mid-year goals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
